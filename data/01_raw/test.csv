,test,category,category_justification,is_shortlisted,is_implemented,short_description,test_procedure,description,why_not_shortlisted,dataset,dataset_source,dataset_size,model,test_source_paper,test_source_code,test_implementation_link,Input_type,test_metric,Solution,Noise
,cough_and_fever,fidelity,it is a simple tree model that demonstrates inconsistencies in explanation \citep{lundberg2018consistent}.,1,1,Can the xAI algorithm detect symmetric binary input features?,train a model such that its response to the two features is exactly the same. The xAI algorithm should detect symmetric features (equal values) and allocate them equal importance.,The trained model's equation is [Cough AND Fever]*80.,-,synthetic_uniform_distribution,-,20000,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever.py,binary,1 if the xAI detects the two features are symmetric. 0 if the difference in importance is above one unit.,,0
,cough_and_fever_10_90,fidelity,it is a simple tree model that demonstrates inconsistencies in explanation due to the tree structure \citep{lundberg2018consistent}.,1,1,Can the xAI algorithm detect that 'Cough' feature is more important than 'Fever'?,train a model with two features with unequal impact on the model. The feature with a higher influence on the output should be detected as more important.,"The trained model's equation is [Cough AND Fever]*80 + [Cough]*10. Cough should be more important than Fever globally. Locally for the case (Fever = yes, Cough = yes) the feature attribution of Cough should be more important.",-,synthetic_uniform_distribution,-,20000,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever_10_90.py,binary,Return 1 if Cough is more important otherwise 0.,,0
,detect_interaction0,fidelity,,,1,Tests wether xAI detects that there is no interaction between inputs ,,,double check if it is redundant,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,,,,
,detect_interaction1,fidelity,,1,1,Tests wether xAI detects that there is simple multiplicative interaction between inputs,use an equation with product terms. detect if two features interact to influence the output,multiple product terms of pairs of features,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"[0,1[ might be another interval let's check on github",,,0
,detect_interaction2,fidelity,,1,1,Tests wether xAI detects 'and' interaction between inputs,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0
,detect_interaction3,fidelity,,1,1,Tests wether xAI detects 'and' interaction between baseline and inputs as well as between inputs,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0
,detect_interaction4,fidelity,,1,1,Tests wether xAI detects 'and' interaction between specific inputs and baseline as well as between inputs,,Combines x' and x*,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0
,Sentiment_Analysis-Word_lvl-BERT,,,,0,-,,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction (Ex: not, very, bad)

local explanation:
we could consider a specific example like not very bad",combining multiple end-user requirements,SST [43],,,,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,Text_word,,,1
,Sentiment_Analysis-Phrase_lvl-BERT,,,,0,-,,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction (Ex: not, very, bad)",,SST [43],,,,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,Text_phrase,,,1
,Image_Classification-ResNet152,,,,0,-,,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction",,ImageNet [12],,,,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,img_segment,higher AUC...,,1
,Recommendation,,,0,0,-,,"Recommendation Task: Fig. 6 shows Archipelago’s result for this task using a state-of-the-art AutoInt model [44] for ad-recommendation. Here, our approach finds a positive interaction between“device_id” and “banner_pos” in the Avazu dataset [1], meaning that the online advertisement model decides the banner position based on user device_id. Note that for this task, there are no ground truth annotations. => for this reason I will not implement it",combining multiple end-user requirements,Avazu dataset [1],,,,tsang2020does,https://github.com/mtsang/archipelago,,Tabular,,,1
,ReLU(x1 + x3 + 1) + ReLU(x2) + 1,,,1,0,-,,Set Attribution Counterexample: is still a new axiom. There is no test but only a demonstration,,-,,,,tsang2020does,https://github.com/mtsang/archipelago,,R,,,0
,x0_plus_x1_distrib_non_uniform_stat_dep,stability,it assesses the impact of slightly changing the inputs \citep{janzing2020feature}.,0,1,Is the xAI able to explain the model correctly despite a non-uniform statistically-dependent distribution of the data?,"check if the explanation change when the distribution change. Check if non-uniform,  statistically dependent distributions affect the explanation",The test demonstrates the effect of data distribution / causal inference.,-,non-uniform and statistically dependent,-,1650,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",returns 1 if the two binary features obtain the same importance,,0
,x0_plus_x1_distrib_non_uniform_stat_indep,stability,it assesses the impact of slightly changing the inputs \citep{janzing2020feature}.,1,1,Is the xAI able to explain the model correctly despite a non-uniform distribution of the data?,Check if the explanation change when the distribution change. Check if non-uniform distributions affect the explanation.,The test demonstrates the effect of data distribution / causal inference.,-,non-uniform and statistically independent,-,10000,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",returns 1 if the two binary features obtain the same importance.,,0
,x0_plus_x1_distrib_uniform_stat_dep,stability,it assesses the impact of slightly changing the inputs \citep{janzing2020feature}.,1,1,Is the xAI able to explain the model correctly despite a statistically-dependent distribution of the data?,Check if the explanation change when the distribution change. Check if statistically dependent distributions affect the explanation.,The test demonstrates the effect of data distribution / causal inference. The example was given in both \citep{hooker2021unrestricted} and \citep{janzing2020feature}.,-,uniform and statistically dependent,-,10000,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",returns 1 if the two binary features obtain the same importance.,,0
,mnist,stress,of the high number of input features. The test is adapted from \citep{covert2020understanding}.,1,1,Is the xAI able to detect all dummy (constant and useless) pixels?,simply train and explain the MLP model globally for every pixel.,The xAI algorithm should detect that important pixels are only in the center of the image.,-,MNIST,https://www.openml.org/d/554,70000,MLP,covert2020understanding,https://github.com/iancovert/sage-experiments/blob/main/experiments/univariate_predictors.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/mnist.py,image,Return the ratio of constant pixels detected as dummy divided by the true number of constant pixels.,,1
,bank,,,0,0,-,,Identifying corrupted features. but i think this is not a relevant test and could be added at a later stage,,Bank http://rstudio-pubs-static.s3.amazonaws.com/19586_40769e2b72aa4558ab9493ec651e4c90.html,,,,covert2020understanding,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,Tabular,,,1
,input correlation,stability,"Adding less than 1 percent of outliers might drop the correlation\footnote{ \url{https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide-2.php}. Therefore stability is the right category (small changes in the setup might heavily affect the explanation).",1,failed to implement,todo,,effect of feature correlations on the xai. Our implementation: effect of feature correlation is grafted onto “A and B or C”:   Add a feature D that perfectly correlates to A ( given the reference datset ) and that is still not used by the trained model (the model can be an if-else function to be safe).  Is the xAI able to give the same explanation despite the high correlation? Score (1) “A as the most important feature” like in A and B or C.  And Score (2) “D is dummy” as in the dummy uni test: 1 - (D_importance / highest_importance) gives a continuous score.  Output average (1) (2).  Another example in kumar2020problems,,synthetic,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,-,,,1
,faithfulness(↑),,,0,0,-,,corr between feature attribution and approximate marginal contribution. Since  a perfect correlation does not mean the best intuitive explanation thus we discard this ambiguous test (n need to mention that in the paper,,synthetic,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,monotonicity(↑),,,0,0,-,,like the other features. its metric is debatable.,,,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,remove-and-retrain (ROAR)(↑),,,0,0,-,,does retrain the model so it is not a test,,,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,GT-Shapley(↑),,,0,0,-,,does the xAI output correlate with the ground truth shapley values. Noooooo !,,,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,infidelity(↓),,,0,0,-,,-,,,,,,liu2021synthetic,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,x0*x1,,,0,0,classical interaction test but might be considered as duplicate of the archipelago tests,,,,,,,,sundararajan2020shapley,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,fooling_perturbation_alg,fragility,"fragility includes all adversarial attacks \citep{ghorbani2019interpretation}.",1,1,Is the xAI affected by an adversarial attack against perturbation-based algorithms?,The xAI algorithms need to explain the following corrupted model (custom function): if the input is from the dataset then the output is from a biased model. if not then the output is from a fair model.,"Model-agnostic xAI algorithms that use feature perturbation methods might be vulnerable to this attack. The adversarial attack exploits a vulnerability to lower the feature importance of a specific feature. Setup: Let's begin by examining the COMPAS data set. This data set consists of defendant information from Broward Couty, Florida. Let's suppose that some adversary wants to mask biased or racist behavior on this data set.",-,COMPAS,https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv,4629,function,lakkaraju2020fool,https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/master/COMPAS_Example.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/fooling_perturbation_alg.py,categorical/scalars,Return 1 if Race is the most important feature despite the adversarial attack. The score decreases while its rank decrease.,,1
,interaction_more_important_than_individuals,fidelity,,1,not a priority,,,you can conclude that the interaction affect the output but not individual features,,,,,,https://www.youtube.com/watch?v=9yIobRrZAyg&ab_channel=TileStats probably a paper behind it todo add link,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,scalar,,,1
,data sparsity,stress,,1,to implement,,,"sensitive to the degree of data sparsity ? sparsity arises naturally when the variables are continuous because it is unlikely that data points share feature values precisely. see https://arxiv.org/pdf/1908.08474.pdf   ",,Diabetes Prediction,,,,sundararajan2020many,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,,"One way to deal with this sensitivity is to smooth the data. We can simulate smoothing within Algorithm 1. When we condition on a set S of features in the computation of CES, we average the prediction over all the training data points that are close to the explicand in each of the features in S; two data points are close in a certain feature if their difference is within a certain fraction of the standard deviation. In our experiments, we use two settings 0.1 and 0.2. Figure 1 shows how different amounts of smoothing change the attributions (see for instance the attributions of the feature  2). hus while smoothing mitigates sensitivity, it is still unclear how much smoothing to do. https://arxiv.org/pdf/1908.08474.pdf ",
,counterexample_dummy_axiom,simplicity,assigning an importance of zero to a dummy feature reflects the model behavior (Fidelity) but also helps the data scientist to quickly understand the model (Simplicity).,1,1,Is the xAI able to detect unused input features?,Train a model with one extra feature B which is a dummy feature.,This is a counter example used in literature to verify that SHAP CES does not satisfy the dummy axiom while BSHAP succeeds in this test.,-,synthetic,-,20000,function,sundararajan2020many,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/dummy_axiom.py,continuous,returns 1 if the dummy feature B obtains null importance.,,
,Failure of Linearity: f1=y ; f2 = x ;  f1 + f3,simplicity,,0,0,Is the explanation consistent with 2 models and their linear combination?,,,Found example might not fit the protocol as it is based on the variation of the distribution,synthetic,,,,sundararajan2020many,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,,,
,Product blurs local explanation,fidelity,,1,to implement,,,limit of shap in locally interpreting correctly using its model agnostic method (interventional) features get the same contribution while it is not true,-,synthetic,,,,kumar2020problems,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,"globally: they should have the same importance. but this is not what we want to test.
locally: Section 3 at the end issue (wrong answer): ""the Shapley value for every feature i is 1 d f(x), regardless of the value xi. Even if, for instance, the magnitude of one of the variables is much higher than the other""",,1
,simple interaction,,,1,not a priority,,,sin(x1 +x2) just an example of pair f interaction was not used to demo any kind of limitation so no need to implement it,-,synthetic,,,,tsang2021interpretable,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,,-,0
,a_and_b_or_c,fidelity,of the same reason as cough and fever 10-90: A's effect on the output is higher than B or C.,1,1,Can the xAI algorithm detect that input feature 'A' is more important than 'B' or 'C'?,The model learns the following equation: A and (B or C). The explanation should prove that A is more important.,"This is a baseline test that the xAI should succeed in all cases. Model: A and (B or C). Goal: make sure that A is more important than B, C. Noise effect: even if the model output is not exactly equal to 1 we still expect the xai to give a correct answer.",-,synthetic,-,20000,XGBRegressor,-,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/a_and_b_or_c.py,binary,If A is the most important feature then return 1. If A is the 2nd most important feature then return 0.5 i.e. 1- (1 / nb of feature more important than A).  If A is the last one: return 0 (completely wrong).,-,0
,Implementation invariance axiom,stability,,1,,,,,-,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,nlp_correctness,,,,,,,todo read https://arxiv.org/pdf/2105.09740.pdf,,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,saliency_map_adv_attack,,,,not a priority,,,see Interpretation of Neural Networks is Fragile (ghorbani et al. 2018),,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,counterfactual_adv_attack,,,,not a priority,,,Counterfactual Explanations Can Be Manipulated (slack singh 2021) - minor changes in the training objective can drastically change counterfactual explanations,,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,gini is wrong,,,1,to implement,,,"Trouble in paradise https://explained.ai/rf-importance/index.html#:~:text=Random%20Forest%20Importances%3F%E2%80%9D-,Trouble%20in%20paradise,-Have%20you%20ever",-,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,foolbox,,,,0,,,"ready to use adv attacks on computer vision tasks probably not unitary and probably not compatible with our protool (not unitary) https://github.com/bethgelab/foolbox  ",,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,test for Anchor,,,,,,,https://arxiv.org/pdf/1907.02509.pdf  Background Section,,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,adv attacks,,,,,,,not targeting our explainers and focus on visual tasks https://github.com/csinva/csinva.github.io/blob/master/_notes/research_ovws/ovw_interp.md#adv-vulnerabilities=,,,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,stress_nb_features,stress, of the high number of tokens.,1,1,Is the xAI able to explain NLP tasks despite a high number of feature values (here 1555 tokens)?,Specific tokens get a high influence on the output as proven in cited papers.,,-,imdb,http://www.aclweb.org/anthology/P11-1015,25000,LogisticRegression,lundberg2017unified,https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/stress_nb_features.py,tokens,"Return 1 if the chosen token is in the top 4. Return 0 if it is in the last 25 percent least important feature.",,
,or_function,fidelity,,0,0,,,does not highlight any unitary problem,,synthetic,,,,lundberg2017unified,https://github.com/slundberg/shap/blob/9411b68e8057a6c6f3621765b89b24d82bee13d4/notebooks/tabular_examples/tree_based_models/Explaining%20a%20simple%20OR%20function.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,encoding_out_of_distribution,fidelity,,1,to implement,is the explanation encoding out-of-distribution predictions using the model?,,if the model is using additional predictions to explain then it might encode out-of-distribution information,-,,,,,jethani2021have,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,local_attribution_product,fidelity,,,,Is the local attribution of a product term correct?,,is it a duplicate of kumar2020problems ?,,ghalebikesabi2021locality,,,,,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,,,,
,correlated_features,fragility,,,1,Tests wether Explainer detects that features are perfectly correlated,Train model on three input features two of which are perfectly correlated. Explainer should only assign high importance to one of the correlated features,,,synthetic,,20000,XGBRegressor,kumar2020problems,,,,Return 1 if importance for one of the correlated features is 0, 0 if importance is equal,,,