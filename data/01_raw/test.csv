test,category,is_fair,is_implemented,short_description,description,why_not_fair,dataset,source_tag,source_code,Input_type,Solution,feature symmetry,Pair Feature Interaction,multiple Feature Interaction,Noise,Global Explanation (Feature importance),Local Explanation (f attribution),Feature Interaction,ml_task_todo
cough_and_fever,fidelity,1,1,Can the xAI detect symmetric binary input features?,[Cough & Fever]*80,,-,lundberg2018consistent,,binary,,1,1,0,0,Cough & Fever have the same importance,Cough & Fever are symmetric features,-,
cough_and_fever_10_90,fidelity,1,1,Can the xAI detect that 'Cough' feature is more important than 'Fever'?,[Cough & Fever]*80 + [Cough]*10: Cough should be more important than Fever,,-,lundberg2018consistent,,binary,,1,1,0,0,Cough should be more important than Fever,"Case (Fever = yes, Cough = yes) Feature Attribution of Cough should be more important",-,
detect_interaction0,fidelity,,1,todo,,double check if it is redundant,,,,,,,,,,,,,
detect_interaction1,fidelity,1,1,todo,multiple product terms of pairs of features,,-,tsang2020does,,"[0,1[ might be another interval let's check on github",,1,1,0,0,-,-,1,
detect_interaction2,fidelity,1,1,todo,still did not understand the diff between x* and x' in F2 and F3,,-,tsang2020does,,"binary (-1,1)",,1,1,1,0,-,-,1,
detect_interaction3,fidelity,1,1,todo,still did not understand the diff between x* and x' in F2 and F3,,-,tsang2020does,,"binary (-1,1)",,1,1,1,0,-,-,1,
detect_interaction4,fidelity,1,1,todo,Combines x' and x*,,-,tsang2020does,,"binary (-1,1)",,1,1,1,0,-,-,1,
Sentiment_Analysis-Word_lvl-BERT,,,0,-,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction (Ex: not, very, bad)",E2E test,SST [43],tsang2020does,,Text_word,,,1,1,1,-,we could consider a specific example like not very bad,,
Sentiment_Analysis-Phrase_lvl-BERT,,,0,-,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction (Ex: not, very, bad)",,SST [43],tsang2020does,,Text_phrase,,,1,1,1,todo see [26] for details,-,,
Image_Classification-ResNet152,,,0,-,"Test what is called ""Interaction Redundancy"" i.e. if with n input I detect the same interaction as with n-1 inputs => clearly for multi feature interaction",,ImageNet [12],tsang2020does,,img_segment,,,1,1,1,higher AUC...,some covid chest x ray... but not sure that the model is indeed learning interaction,,
Recommendation,,0,0,-,"Recommendation Task: Fig. 6 shows Archipelago’s result for this task using a state-of-the-art AutoInt model [44] for ad-recommendation. Here, our approach finds a positive interaction between“device_id” and “banner_pos” in the Avazu dataset [1], meaning that the online advertisement model decides the banner position based on user device_id. Note that for this task, there are no ground truth annotations. => for this reason I will not implement it",E2E test,Avazu dataset [1],tsang2020does,,Tabular,,,1,1,1,,,,
ReLU(x1 + x3 + 1) + ReLU(x2) + 1,,1,0,-,Set Attribution Counterexample: is still a new axiom. There is no test but only a demonstration,,-,tsang2020does,,R,,1,1,0,0,-,-,,
x0_plus_x1_distrib_non_uniform_stat_dep,stability,1,1,Is the xAI able to explain the model correctly despite a non-uniform statistically-dependent distribution of the data?,demonstrate the effect of data distribution / causal inference,,non-uniform /statistically independent,janzing2020feature,,"{0,1}",,1,0,0,0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,
x0_plus_x1_distrib_non_uniform_stat_indep,stability,1,1,Is the xAI able to explain the model correctly despite a non-uniform distribution of the data?,demonstrate the effect of data distribution / causal inference,,non-uniform /statistically independent,janzing2020feature,,"{0,1}",,1,0,0,0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,
x0_plus_x1_distrib_uniform_stat_dep,stability,1,1,Is the xAI able to explain the model correctly despite a statistically-dependent distribution of the data?,demonstrate the effect of data distribution / causal inference. The example was given in both https://arxiv.org/pdf/1905.03151.pdf and \cite{janzing2020feature},,non-uniform /statistically independent,janzing2020feature,,"{0,1}",,1,0,0,0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,
mnist,stress,1,1,Is the xAI able to detect all dummy (constant and useless) pixels?,We agree that important pixels are in the center of the image,,MNIST,covert2020understanding,,image,,,1,1,1,"we expect the model to not show 0 importance to the pixels in the border of the image. Able to identify the top important features and top less important features. P.S. I did not implement the score function because I do not have the exact values from the plot. todo use that tool online to get the values or check their repo",-,,
bank,,0,0,-,Identifying corrupted features. but i think this is not a relevant test and could be added at a later stage,,Bank http://rstudio-pubs-static.s3.amazonaws.com/19586_40769e2b72aa4558ab9493ec651e4c90.html,covert2020understanding,,Tabular,,,,,1,,,,
input correlation,stability,1,failed to implement,todo,effect of feature correlations on the xai. another example in kumar2020problems,,synthetic,liu2021synthetic,,-,,,,,1,,,,
faithfulness(↑),,0,0,-,corr between feature attribution and approximate marginal contribution. Since  a perfect correlation does not mean the best intuitive explanation thus we discard this ambiguous unit test (n need to mention that in the paper,,synthetic,liu2021synthetic,,,,,,,,,,,
monotonicity(↑),,0,0,-,like the other features. its metric is debatable.,,,liu2021synthetic,,,,,,,,,,,
remove-and-retrain (ROAR)(↑),,0,0,-,does retrain the model so it is not a unit test,,,liu2021synthetic,,,,,,,,,,,
GT-Shapley(↑),,0,0,-,does the xAI output correlate with the ground truth shapley values. Noooooo !,,,liu2021synthetic,,,,,,,,,,,
infidelity(↓),,0,0,-,-,,,liu2021synthetic,,,,,,,,,,,
x0*x1,,1,not a priority,todo after acceptance,,,,did not find a paper implementing that yet. but it is a quite popular example,,,,1,1,0,,-,-,1,
fooling_perturbation_alg,fragility,1,1,Is the xAI affected by an adversarial attack against perturbation-based algorithms?,"exploit a vulnerability in model-agnostic xai that use feature perturbation: adversarial attack to lower the feature importance of a specific feature
### Setup
Let's begin by examining the COMPAS data set. This data set consists of defendent information from Broward Couty, Florida. Let's suppose that some adversary wants to _mask_ baised or racist behavior on this data set.",,"https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv
https://www.kaggle.com/datasets/danofer/compass",lakkaraju2020fool,https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/master/COMPAS_Example.ipynb,categorical/scalars,,0,0,0,1,"Race should be the most important feature
is the f importance of Race as the correct value or is it affected by the adversarial attack
how to: corrupt out of distribution predictions => works only on model agnostic xai
adversarial black box provided: if perturbed return prediction from fair model
                                else return the prediction from the real biased model
https://github.com/SinaMohseni/Awesome-XAI-Evaluation/blob/master/tables/main-table.csv
",-,-,
interaction_more_important_than_individuals,fidelity,1,not a priority,,you can conclude that the interaction affect the output but not individual features,,,https://www.youtube.com/watch?v=9yIobRrZAyg&ab_channel=TileStats probably a paper behind it todo add link,,scalar,,0,1,0,1,,,,
data sparsity,stress,1,to implement,,"sensitive to the degree of data sparsity ? sparsity arises naturally when the variables are continuous because it unlikely that data points share feature values precisely.
 see https://arxiv.org/pdf/1908.08474.pdf   ",,Diabetes Prediction,sundararajan2020many,,continuous,"One way to deal with this sensitivity is to smooth the data. We can simulate smoothing within Algorithm 1. When we condition on a set S of features in the computation of CES, we average the prediction over all the training data points that are close to the explicand in each of the features in S; two data points are close in a certain feature if their difference is within a certain fraction of the standard deviation. In our experiments, we use two settings 0.1 and 0.2. Figure 1 shows how different amounts of smoothing change the attributions (see for instance the attributions of the feature  2). hus while smoothing mitigates sensitivity, it is still unclear how much smoothing to do. https://arxiv.org/pdf/1908.08474.pdf ",,,,,,,,
counterexample_dummy_axiom,simplicity,1,1,Is the xAI able to detect unused input features?,counter example that SHAP CES do not satisfy the dummy axiom. BSHAP succeed in this test,,synthetic,sundararajan2020many,,continuous,,,,,,,,,
Failure of Linearity: f1=y ; f2 = x ;  f1 + f3,simplicity,1,0,Is the explanation consistent with 2 models and their linear combination?,Found example might not fit the protocol as it is based on the variation of the distribution,,synthetic,sundararajan2020many,,continuous,,,,,,,,,,
Product blurs local explanation,fidelity,1,to implement,,limit of shap in locally interpreting correctly using its model agnostic method (interventional) features get the same contribution while it is not true,,synthetic,kumar2020problems,,continuous,,1,1,1,1,they should have the same importance. but this is not what we want to test,"Section 3 at the end issue (wrong answer): ""the Shapley value for every feature i is 1 d f(x), regardless of the value xi. Even if, for instance, the magnitude of one of the variables is much higher than the other""",-,
simple interaction,,1,not a priority,,sin(x1 +x2) just an example of pair f interaction was not used to demo any kind of limitation so no need to implement it,,synthetic,tsang2021interpretable,,continuous,-,1,1,0,0,-,-,x1 and x2 interact,
a_and_b_or_c,fidelity,1,1,Can the xAI detect that input feature 'A' is more important than 'B' or 'C'?,"This is a baseline test that the xAI should succeed in all cases. Model: A and (B or C). Goal: make sure that A is more important than B, C. Noise: even if the model output is not == 1. still we expect the xai to give a correct answer => no noise",,synthetic,-,-,binary,-,,,,,,,,
Implementation invariance axiom,stability,1,,,,,,,,,,,,,,,,,
nlp_correctness,,,,,todo read https://arxiv.org/pdf/2105.09740.pdf,,,,,,,,,,,,,,
saliency_map_adv_attack,,,not a priority,,see Interpretation of Neural Networks is Fragile (ghorbani et al. 2018),,,,,,,,,,,,,,
counterfactual_adv_attack,,,not a priority,,Counterfactual Explanations Can Be Manipulated (slack singh 2021) - minor changes in the training objective can drastically change counterfactual explanations,,,,,,,,,,,,,,
gini is wrong,,1,to implement,,"Trouble in paradise https://explained.ai/rf-importance/index.html#:~:text=Random%20Forest%20Importances%3F%E2%80%9D-,Trouble%20in%20paradise,-Have%20you%20ever",,,,,,,,,,,,,,
foolbox,,,0,,"ready to use adv attacks on computer vision tasks probably not unitary and probably not compatible with our protool (not unitary) https://github.com/bethgelab/foolbox  ",,,,,,,,,,,,,,
unit test for Anchor,,,,,https://arxiv.org/pdf/1907.02509.pdf  Background Section,,,,,,,,,,,,,,
adv attacks,,,,,not targeting our explainers and focus on visual tasks https://github.com/csinva/csinva.github.io/blob/master/_notes/research_ovws/ovw_interp.md#adv-vulnerabilities=,,,,,,,,,,,,,,
stress_nb_features,stress,1,1,is the xAI able to explain NLP tasks despite a high number of input features (here 1555 tokens)?,,,imdb,lundberg2017unified,https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.ipynb,tokens,,,,,,,Test if best is in the top most useful tokens,,
or_function,fidelity,0,0,,does not highlight any unitary problem,,synthetic,lundberg2017unified,https://github.com/slundberg/shap/blob/9411b68e8057a6c6f3621765b89b24d82bee13d4/notebooks/tabular_examples/tree_based_models/Explaining%20a%20simple%20OR%20function.ipynb,,,,,,,,,,
encoding_out_of_distribution,fidelity,1,to implement,is the explanation encoding out-of-distribution predictions using the model?,if the model is using additional predictions to explain then it might encode out-of-distribution information,,,jethani2021have,,,,,,,,,,,
local_attribution_product,fidelity,,,Is the local attribution of a product term correct?,is it a duplicate of kumar2020problems ?,,ghalebikesabi2021locality,,,,,,,,,,,
