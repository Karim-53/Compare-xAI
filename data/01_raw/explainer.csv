explainer,is_documented,is_implemented,source_paper_tag,description,source_code,implementation_link,supported_models,outputs,output_attribution,output_importance,output_interaction,supported_model_model_agnostic,supported_model_tree_based,supported_model_neural_network
exact_shapley_values,1,1,shapley1953quota,"is a permutation-based xAI algorithm following a game theory approach: Iteratively Order the features randomly, then add them to the input one at a time following this order, and calculate their expected marginal contribution \citep{sundararajan2020many}. The output is unique given a set of constrains defined in the original paper.",https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,"""['model_agnostic']""","""['importance']""",False,True,False,True,True,True
kernel_shap,0.75,1,lundberg2017unified,it approximates the Shapley values with a constant noise \citep{janzing2020feature}.,https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,"""['model_agnostic']""","""['attribution','importance']""",True,True,False,True,True,True
tree_shap,0.5,1,lundberg2018consistent,accurately compute the shap values using the structure of the tree model.,,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,"""['tree_based']""","""['attribution','importance']""",True,True,False,False,True,False
saabas,0.75,1,-,explain tree based models by decomposing each prediction into bias and feature contribution components,https://github.com/andosa/treeinterpreter/,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/saabas.py,"""['tree_based']""","""['attribution','importance']""",True,True,False,False,True,False
lime,0.5,1,ribeiro2016should,it explains the model locally by generating an interpretable model approximating the original one.,,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/lime.py,"""['model_agnostic']""","""['attribution','importance']""",True,True,False,True,True,True
archipelago,1.0,1,tsang2020does,"separate the input features into sets. all features inside a set interact, and there is no interaction outside a set. ArchAttribute is an interaction attribution method. ArchDetect is the corresponding interaction detector.",https://github.com/mtsang/archipelago/blob/main/experiments/1.%20archdetect/1.%20synthetic_performance.ipynb,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/archipelago.py,"""['model_agnostic']""","""['interaction']""",False,False,True,True,True,True
Difference,0.0,,,,,,,,,,,,,
anova,0.75,0.75,,"Fisher (1921) also developed one of the foundations of statistical analysis called Analysis of Variance (ANOVA) including two-way ANOVA (Fisher, 1925),
Multi-way ANOVA exists to detect interactions of higher-orders combinations, not just between pairs of features.
A significant problem with individual testing methods is that they require an exponentially growing number of
tests as the desired interaction order increases. Not only is this approach intractable, but also has a high
chance of generating false positives or a high false discovery rate (Benjamini and Hochberg, 1995) that arises
from multiple testing. see https://arxiv.org/pdf/2103.03103.pdf",,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/anova.py,"""['model_agnostic']""","""['interaction']""",False,False,True,True,True,True
IG: Integrated Gradients,0.25,to implement,,"IG is an analog of the Aumann-Shapley method from cost-sharing [4]. We will discuss the sense in which IG is an extension of the Shapley value in Section 4.1 https://arxiv.org/pdf/1908.08474.pdf

IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018).  see  https://arxiv.org/pdf/2103.03103.pdf

The Integrated Gradients (IG) (Sundararajan et al., 2017) is an extremely
popular path integration method used for feature attribution. It obeys a host of axioms similar to
Shapley index while also being applicable to continuous input variables. It uses the following equation
to integrate over the model’s decision output F from a baseline input x’ to the input of interest x
with a linear interpolation.Because it works on any differentiable model, it has become a staple for interpretation in some deep learning communities. https://arxiv.org/pdf/2103.03103.pdf",,,,,,,,,,
IH: Integrated Hessians,0.25,,,"The Integrated Hessians (IH) method (Janizek et al., 2020) extends this to interactions by applying the method to itself. In this way, the method describes how important j’s importance is to i, but is more easily understood as the feature interaction.
This method is able to enjoy a lot of the axioms coming from IG, but one of its major drawbacks is a result of not formalizing interactions and main effects. The term IHi,i is more like a residual term instead of the main effect, which can make it difficult to identify how much of the actual effects are being properly captured by this method https://arxiv.org/pdf/2103.03103.pdf",,,,,,,,,,
MAHE: Model-Agnostic Hierarchical Explanations,0.25,,,"Model-Agnostic Hierarchical Explanations (MAHE) (Tsang et al., 2018b) trains a surrogate explainer models for interaction detection and attribution in a method similar to LIME. MAHE’s explanations are then fed through a NID model and final GAM to get the attributions. This methodology allows MAHE to provide explanations relatively quickly by not needing to look at the overall feature space. Because MAHE is trained on overlapping feature sets, MAHE can fail a lot of the desirable axioms satisfied by these other attribution methods. https://arxiv.org/pdf/2103.03103.pdf",,,,,,,,,,
neural_interaction_detection,0.75,0.5,,"The modern work of Neural Interaction Detection (NID) Tsang et al. (2018a) circumvented this
problem by using the powerful inductions of feedforward neural networks. This work takes advantage of the specific structure of neural networks to avoid needing to specifying large numbers of
interactions while still not focusing on individual testing. The method traces high-strength weights
from features to common hidden units as a way to automatically detect which features have high
dependencies on one another. Later work Tsang et al. (2020a) uses NID to train a new network with
additional ‘cross-features’ from the detected interactions to actually boost the deep networks performance. Given the popularity of deep learning, the prospect of improving deep networks’ performance
through understanding feature interactions warrants further investigation into this re-emerging topic
of feature interactions. see https://arxiv.org/pdf/2103.03103.pdf ",,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/neural_interaction_detection.py,"""['neural_network']""","""['interaction']""",False,False,True,False,False,True
shap_interaction,0.75,1,owen1972multilinear,SI: Shapley Interaction Index.,,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_interaction.py,"""['model_agnostic']""","""['interaction']""",False,False,True,True,True,True
shapley_taylor_interaction,0.75,1,sundararajan2020shapley,STI: Shapley Taylor Interaction Index.,,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shapley_taylor_interaction.py,"""['model_agnostic']""","""['interaction']""",False,False,True,True,True,True
SCD: Sampling Contextual Decomposition,0.0,,,,,,,,,,,,,
SOC: Sampling Occlusion,0.0,,,,,,,,,,,,,
CD: Contextual Decomposition,0.25,,,"variant of SCD [35,42], see tsang2020does
Agglomerative Contextual Decomposition: Agglomerative Contextual Decomposition (ACD) (Singh et al., 2019) is a hierarchical extension of Contextual Decomposition (Murdoch et al., 2018) which was originally designed for LSTMs. This method extended the additive decomposition to NNs in general by adding technical solutions for the method to work on convolutions and max pooling, both common operations in CNNs. The method decomposes each layer as:
gi(x) = βi(x) + γi(x)
Each gi denotes a layer of the network and where β denotes the contribution only by the features of interest and γ denotes the residual contribution for the model. Thus, the β values can be seen as the interaction effect of a certain group of features when it is the final β value from the logit/ prediction layer. The method then accumulates these scores into a tree hierarchy over the input. Because this work was designed for text and images, this looks something like a tree of word subsets with their corresponding polarity or image patches with their corresponding classes. A primary weakness is this method also fails many axiomatic interaction scores because of its formulation. A minor weakness is the computation time required by the iterative, hierarchical, full-explanation approach. This is because a partial hierarchy could be computed if desired and many pieces of this framework are flexible for quicker or even different inference.
",,,,,,,,,,
sage,1.0,1,covert2020understanding,"Compute feature importance based on Shapley value but faster. The features that are most critical for the model to make good predictions will have large importance and only features that make the model's performance worse will have negative values.
Disadvantage: The convergence of the algorithm depends on 2 parameters: `thres` and `gap`. The algorithm can be trapped in a potential infinite loop if we do not fine tune them.",https://github.com/iancovert/sage,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/sage_explainer.py,"""['model_agnostic']""","""['importance']""",False,True,False,True,True,True
Mean Importance,0.25,,,feature importance cited in covert2020understanding,,,,,,,,,,
Feature Ablation,0.25,,,feature importance cited in covert2020understanding,,,,,,,,,,
Univariate Predictors,0.25,,,feature importance cited in covert2020understanding,,,,,,,,,,
SHAPR,0.25,to implement,harris2021joint,Computes the shapley values for interactions up to a user specified power k. Should lead to more representative importance values then standard shapley but takes longer to run,https://github.com/harris-chris/joint-shapley-values,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shapr.py,,,True,True,True,True,True,True
maple,0.75,1,plumb2018model,is a supervised neighborhood approach that combines ideas from local linear models and ensembles of decision trees \citep{plumb2018model}.,,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/maple.py,"""['model_agnostic']""","""['attribution','importance']""",True,True,False,True,True,True
L2X,0.25,,,https://github.com/Jianbo-Lab/L2X also implemented in liu2021synthetic see https://github.com/abacusai/xai-bench,,,,,,,,,,
breakdown,0.25,to implement,staniak2018explanations,approximate the shapley values (see https://christophm.github.io/interpretable-ml-book/shapley.html#software-and-alternatives-4= ) implemented in liu2021synthetic see https://github.com/abacusai/xai-bench,,,,,,,,,,
baseline_random,1.0,1,liu2021synthetic,Output a random explanation. It is not a real explainer. It helps measure the baseline score and processing time.,created,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/explainer_superclass.py,"""['model_agnostic']""","""['attribution','importance','interaction']""",True,True,True,True,True,True
Uniform,0.25,,Us,output uniform feature attribution,,,,,,,,,,
Zero,0.25,,Us,output zero feature attribution,,,,,,,,,,
DeepLIFT,0.0,,,,,,,,,,,,,
Shapley Regression,0.0,,,,,,,,,,,,,
BShap: Baseline Shapley,0.25,to implement,sundararajan2020many,Is unique (in contrast with shapley values) need a baseline to provide local explanation,,,,,,,,,,
RBShap: Random Baseline Shapley,0.25,to implement,sundararajan2020many,"BShap that takes three inputs: An explicand x, a function f, and a distribution D. The attributions are the expected BShap values, where the baseline x' is drawn randomly according to the distribution D. This approach is implicit in [10] (see Equation 11). https://arxiv.org/pdf/1908.08474.pdf",,,,,,,,,,
QII,0.25,,,see kumar2020problems,,,,,,,,,,
FAE,0.25,,,see kumar2020problems,,,,,,,,,,
TCAV,0.25,,,"Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. see  https://arxiv.org/pdf/2103.03103.pdf   do not consider f Interaction",,,,,,,,,,
AG,0.25,,,"Additive Groves (AG) method (Sorokina et al., 2008). This method is notable
because it detects interactions using high-performance random forest models while accounting for the
non-additive definition of feature interaction. This essentially allows the detected feature interaction
to be unrestricted to a functional form (e.g. multiplication xixj ) which was a big step forward at
the time. AG tests each of these interactions by comparing two regression trees, one that fits all
interactions, and the other that has the interaction of interest forcibly removed. The main issue
with this method is it exacerbates the slowness of individual testing methods which require many
tests by forcing a complex model to be trained for each test.
https://arxiv.org/pdf/2103.03103.pdf ",,,,,,,,,,
permutation,,1,-,is a shuffle-based feature importance. It permutes the input data and compares it to the normal prediction,https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,,,True,True,False,True,True,True
partition,,1,lundberg2017unified,Partition SHAP approximates the Shapley values using a hierarchy of feature coalitions.,https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,,,True,True,False,True,True,True
permutation_partition,,1,-,is a combination of permutation and partition algorithm from shap.,https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,,,True,True,False,True,True,True
tree_shap_approximation,,1,-,is a faster implementation of shap reserved for tree based models.,https://github.com/slundberg/shap,https://github.com/Karim-53/Compare-xAI/blob/main/explainers/shap_explainer.py,,,True,True,False,False,True,False
todo,,0,,,https://arxiv.org/pdf/2006.04750.pdf,,,,,,,,,
ablation f importance,,,,,https://captum.ai/api/feature_ablation.html  https://deepai.org/publication/randomized-ablation-feature-importance,,,,,,,,,
Morris Sensitivity Analysis,,,,,see https://github.com/interpretml/interpret,,,,,,,,,
lofo f importance,,,,,to implement. it retrain the model   https://github.com/aerdem4/lofo-importance,,,,,,,,,
ACV and ACVTree,,to implement,,,https://github.com/salimamoukou/acv00   It provides a better estimation of Shapley Values for tree-based model (more accurate than path-dependent TreeSHAP). It also proposes new Shapley Values that have better local fidelity. and the correct way of computing Shapley Values of categorical variables after encoding (eg. One Hot or Dummy etc.),,,,,,,,,
shap wrong local explanation,,to implement,,,see fig 1  in https://arxiv.org/pdf/2106.14648.pdf,,,,,,,,,
shap is enable to explain dummy features correctly (one hot encoding),,to implement,see https://arxiv.org/pdf/2106.03820.pdf,,,,,,,,,,,
joint_shapley,1,1,,,,,,,,,,,,
