Unnamed: 0,test,category,is_shortlisted,is_implemented,short_description,description,why_not_shortlisted,dataset,dataset_source,dataset_size,model,test_source_paper,test_source_code,test_implementation_link,Input_type,test_metric,Solution,feature symmetry,Pair Feature Interaction,multiple Feature Interaction,Noise,Global Explanation (Feature importance),Local Explanation (f attribution),Feature Interaction,ml_task_todo,Unnamed: 26,Unnamed: 27
,cough_and_fever,fidelity,1.0,1,Can the xAI detect symmetric binary input features?,Model: [Cough & Fever]*80,-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever.py,binary,1 if the xAI detect the 2 features as symetric. 0 if the difference in importance is above one unit.,,1.0,1.0,0.0,0.0,Cough & Fever have the same importance,Cough & Fever are symmetric features,-,,,
,cough_and_fever_10_90,fidelity,1.0,1,Can the xAI detect that 'Cough' feature is more important than 'Fever'?,Model: [Cough & Fever]*80 + [Cough]*10: Cough should be more important than Fever,-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever_10_90.py,binary,"1 if Cough is more important, else 0",,1.0,1.0,0.0,0.0,Cough should be more important than Fever,"Case (Fever = yes, Cough = yes) Feature Attribution of Cough should be more important",-,,,
,detect_interaction1,fidelity,1.0,1,todo,multiple product terms of pairs of features,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"[0,1[ might be another interval let's check on github",,,1.0,1.0,0.0,0.0,-,-,1,,,
,detect_interaction2,fidelity,1.0,1,todo,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,1.0,1.0,1.0,0.0,-,-,1,,,
,detect_interaction3,fidelity,1.0,1,todo,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,1.0,1.0,1.0,0.0,-,-,1,,,
,detect_interaction4,fidelity,1.0,1,todo,Combines x' and x*,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,1.0,1.0,1.0,0.0,-,-,1,,,
,x0_plus_x1_distrib_non_uniform_stat_dep,stability,1.0,1,Is the xAI able to explain the model correctly despite a non-uniform statistically-dependent distribution of the data?,demonstrate the effect of data distribution / causal inference,-,non-uniform /statistically dependent,-,1650.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,1.0,0.0,0.0,0.0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,,,
,x0_plus_x1_distrib_non_uniform_stat_indep,stability,1.0,1,Is the xAI able to explain the model correctly despite a non-uniform distribution of the data?,demonstrate the effect of data distribution / causal inference,-,non-uniform /statistically independent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,1.0,0.0,0.0,0.0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,,,
,x0_plus_x1_distrib_uniform_stat_dep,stability,1.0,1,Is the xAI able to explain the model correctly despite a statistically-dependent distribution of the data?,demonstrate the effect of data distribution / causal inference. The example was given in both https://arxiv.org/pdf/1905.03151.pdf and \cite{janzing2020feature},-,uniform /statistically dependent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,1.0,0.0,0.0,0.0,should get equal importance despite the distribution,should get symmetric feature attribution despite the distribution,,,,
,mnist,stress,1.0,1,Is the xAI able to detect all dummy (constant and useless) pixels?,We agree that important pixels are in the center of the image,-,MNIST,,,,covert2020understanding,https://github.com/iancovert/sage-experiments/blob/main/experiments/univariate_predictors.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,image,,,,1.0,1.0,1.0,we expect the model to not show 0 importance to the pixels in the border of the image. Able to identify the top important features and top less important features. P.S. I did not implement the score function because I do not have the exact values from the plot. todo use that tool online to get the values or check their repo,-,,,,
,fooling_perturbation_alg,fragility,1.0,1,Is the xAI affected by an adversarial attack against perturbation-based algorithms?,"exploit a vulnerability in model-agnostic xai that use feature perturbation: adversarial attack to lower the feature importance of a specific feature ### Setup: Let's begin by examining the COMPAS data set. This data set consists of defendent information from Broward Couty, Florida. Let's suppose that some adversary wants to _mask_ baised or racist behavior on this data set.",,https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv  https://www.kaggle.com/datasets/danofer/compass,,,,lakkaraju2020fool,https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/master/COMPAS_Example.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,categorical/scalars,,,0.0,0.0,0.0,1.0,Race should be the most important feature is the f importance of Race as the correct value or is it affected by the adversarial attack. How to: corrupt out of distribution predictions => works only on model agnostic xai.  Adversarial black box provided: (if perturbed return prediction from fair model    else return the prediction from the real biased model)   https://github.com/SinaMohseni/Awesome-XAI-Evaluation/blob/master/tables/main-table.csv,-,-,,,
,counterexample_dummy_axiom,simplicity,1.0,1,Is the xAI able to detect unused input features?,counter example that SHAP CES do not satisfy the dummy axiom. BSHAP succeed in this test,,synthetic,,,,sundararajan2020many,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,,,,,,,,,,,,
,a_and_b_or_c,fidelity,1.0,1,Can the xAI detect that input feature 'A' is more important than 'B' or 'C'?,"This is a baseline test that the xAI should succeed in all cases. Model: A and (B or C). Goal: make sure that A is more important than B, C. Noise: even if the model output is not == 1. still we expect the xai to give a correct answer => no noise. Score function: if A is the most important feature → return 1.		If A is the 2nd most important feature → return 0.5     i.e. 1- (1 / nb of feature more important than A).  If A is the last one: return 0 (completely wrong)",,synthetic,,,,-,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,binary,,-,,,,,,,,,,
,stress_nb_features,stress,1.0,1,is the xAI able to explain NLP tasks despite a high number of input features (here 1555 tokens)?,,-,imdb,http://www.aclweb.org/anthology/P11-1015,,LogisticRegression,lundberg2017unified,https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,tokens,"1 if the chosen token is in the top 4.
0 if it is in the last 25% least important feature",,,,,,,Test if best is in the top most useful tokens,,,,
