Unnamed: 0,test,category,category_justification,is_shortlisted,is_implemented,short_description,test_procedure,description,why_not_shortlisted,dataset,dataset_source,dataset_size,model,test_source_paper,test_source_code,test_implementation_link,Input_type,test_metric,Solution,Noise
,cough_and_fever,fidelity,it is a simple tree model that demonstrate inconsistencies in explanation \citep{lundberg2018consistent}.,1.0,1,Can the xAI algorithm detect symmetric binary input features?,train a model such that its response to the two features is exactly the same. The xAI algorithm should detect symmetric features (equal values) and allocate them equal importance.,The trained model's equation is [Cough AND Fever]*80.,-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever.py,binary,1 if the xAI detect the two features are symmetric. 0 if the difference in importance is above one unit.,,0.0
,cough_and_fever_10_90,fidelity,it is a simple tree model that demonstrate inconsistencies in explanation due to the tree structure \citep{lundberg2018consistent}.,1.0,1,Can the xAI algorithm detect that 'Cough' feature is more important than 'Fever'?,train a model with two features with unequal impact on the model. The feature with a higher influence on the output should be detected more important.,"The trained model's equation is [Cough AND Fever]*80 + [Cough]*10. Cough should be more important than Fever globally. Locally for the case (Fever = yes, Cough = yes) the feature attribution of Cough should be more important.",-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever_10_90.py,binary,Return 1 if Cough is more important otherwise 0.,,0.0
,detect_interaction1,fidelity,,1.0,1,todo,use an equation with product terms. detect if two features interact to influence the output,multiple product terms of pairs of features,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"[0,1[ might be another interval let's check on github",,,0.0
,detect_interaction2,fidelity,,1.0,1,todo,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,detect_interaction3,fidelity,,1.0,1,todo,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,detect_interaction4,fidelity,,1.0,1,todo,,Combines x' and x*,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,x0_plus_x1_distrib_non_uniform_stat_indep,stability,it assesses the impact of slightly changing the inputs \citep{janzing2020feature}.,1.0,1,Is the xAI able to explain the model correctly despite a non-uniform distribution of the data?,Check if the explanation change when the distribution change. Check if non-uniform distributions affect the explanation.,The test demonstrate the effect of data distribution / causal inference.,-,non-uniform and statistically independent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",returns 1 if the two binary features obtain the same importance.,,0.0
,x0_plus_x1_distrib_uniform_stat_dep,stability,"To assess the impact of changing the inputs of f... This way, we are able to talk about a hypothetical scenario where the inputs are changed compared to the true features \citep{janzing2020feature}.",1.0,1,Is the xAI able to explain the model correctly despite a statistically-dependent distribution of the data?,Check if the explanation change when the distribution change. Check if statistically dependent distributions affect the explanation.,The test demonstrate the effect of data distribution / causal inference. The example was given in both \citep{hooker2021unrestricted} and \citep{janzing2020feature}.,-,uniform and statistically dependent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",returns 1 if the two binary features obtain the same importance.,,0.0
,mnist,stress,of the high number of input features. The test is adapted from \citep{covert2020understanding}.,1.0,1,Is the xAI able to detect all dummy (constant and useless) pixels?,simply train and explain the MLP model globally for every pixel.,The xAI algorithm should detect that important pixels are only in the center of the image.,-,MNIST,https://www.openml.org/d/554,70000.0,MLP,covert2020understanding,https://github.com/iancovert/sage-experiments/blob/main/experiments/univariate_predictors.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/mnist.py,image,Return the ratio of constant pixels detected as dummy divided by the true number of constant pixels.,,1.0
,fooling_perturbation_alg,fragility,fragility includes all adversarial attacks \citep{ghorbani2019interpretation}.,1.0,1,Is the xAI affected by an adversarial attack against perturbation-based algorithms?,The xAI algorithms need to explain the following corrupted model (custom function): if the input is from the dataset then the output is from a biased model. if not then the output is from a fair model.,"Model-agnostic xAI algorithms that use feature perturbation methods might be vulnerable to this attack. The adversarial attack exploits a vulnerability to lower the feature importance of a specific feature. Setup: Let's begin by examining the COMPAS data set. This data set consists of defendant information from Broward Couty, Florida. Let's suppose that some adversary wants to mask biased or racist behavior on this data set.",-,COMPAS,https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv,4629.0,function,lakkaraju2020fool,https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/master/COMPAS_Example.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/fooling_perturbation_alg.py,categorical/scalars,Return 1 if Race is the most important feature despite the adversarial attack. Score decreases while its rank decrease.,,1.0
,counterexample_dummy_axiom,simplicity,assigning an importance of zero to dummy feature reflect the model behavior (Fidelity) but also helps the data scientist to quickly understand the model (Simplicity).,1.0,1,Is the xAI able to detect unused input features?,Train a model with one extra feature B that is dummy.,This is a counter example used in literature to verify that SHAP CES do not satisfy the dummy axiom while BSHAP succeed in this test.,-,synthetic,-,20000.0,function,sundararajan2020many,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/dummy_axiom.py,continuous,returns 1 if the dummy feature B obtain a null importance.,,
,a_and_b_or_c,fidelity,of the same reason as cough and fever 10-90: A's effect on the output is higher than B or C.,1.0,1,Can the xAI algorithm detect that input feature 'A' is more important than 'B' or 'C'?,The model learns the following equation: A and (B or C). The explanation should prove that A is more important.,"This is a baseline test that the xAI should succeed in all cases. Model: A and (B or C). Goal: make sure that A is more important than B, C. Noise effect: even if the model output is not exactly equal to 1 still we expect the xai to give a correct answer.",-,synthetic,-,20000.0,XGBRegressor,-,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/a_and_b_or_c.py,binary,If A is the most important feature then return 1. If A is the 2nd most important feature then return 0.5 i.e. 1- (1 / nb of feature more important than A).  If A is the last one: return 0 (completely wrong).,-,0.0
,stress_nb_features,stress, of the high number of tokens.,1.0,1,Is the xAI able to explain NLP tasks despite a high number of feature values (here 1555 tokens)?,Specific tokens get a high influence on the output as proven in cited papers.,,-,imdb,http://www.aclweb.org/anthology/P11-1015,25000.0,LogisticRegression,lundberg2017unified,https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/stress_nb_features.py,tokens,Return 1 if the chosen token is in the top 4. Return 0 if it is in the last 25 percent least important feature.,,
