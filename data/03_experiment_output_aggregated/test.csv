Unnamed: 0,test,category,category_justification,is_shortlisted,is_implemented,short_description,test_procedure,description,why_not_shortlisted,dataset,dataset_source,dataset_size,model,test_source_paper,test_source_code,test_implementation_link,Input_type,test_metric,Solution,Noise
,cough_and_fever,fidelity,Title of Figure 1:  Two simple tree models that demonstrate inconsistencies \citep{lundberg2018consistent},1.0,1,Can the xAI detect symmetric binary input features?,train a model with shuch that it's response to 2 feature is exactly the same. the xAi should detect symetric features (equal values) and allocate them equal importance.,trained model's equation: [Cough AND Fever]*80,-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever.py,binary,1 if the xAI detect the 2 features as symetric. 0 if the difference in importance is above one unit.,,0.0
,cough_and_fever_10_90,fidelity,Title of Figure 1:  Two simple tree models that demonstrate inconsistencies \citep{lundberg2018consistent},1.0,1,Can the xAI detect that 'Cough' feature is more important than 'Fever'?,train a model with 2 features with inequal impact on the model. Features with a higher influence on the output should be detected more important,"Trained model's equation: [Cough & Fever]*80 + [Cough]*10: Cough should be more important than Fever globally.
locally: Case (Fever = yes, Cough = yes) Feature Attribution of Cough should be more important.",-,synthetic_uniform_distribution,-,20000.0,XGBRegressor,lundberg2018consistent,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/cough_and_fever_10_90.py,binary,"1 if Cough is more important, else 0",,0.0
,detect_interaction1,fidelity,,1.0,1,todo,use an equation with product terms. detect if 2 features interact to influence the output,multiple product terms of pairs of features,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"[0,1[ might be another interval let's check on github",,,0.0
,detect_interaction2,fidelity,,1.0,1,todo,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,detect_interaction3,fidelity,,1.0,1,todo,,still did not understand the diff between x* and x' in F2 and F3,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,detect_interaction4,fidelity,,1.0,1,todo,,Combines x' and x*,,synthetic_uniform_distribution,-,,function,tsang2020does,https://github.com/mtsang/archipelago,https://github.com/Karim-53/Compare-xAI/tree/main/tests/pair_interaction.py,"binary (-1,1)",,,0.0
,x0_plus_x1_distrib_non_uniform_stat_dep,stability,"To assess the impact of changing the inputs of f... This way, we are able to talk about a hypothetical scenario where the inputs are changed compared
to the true features. \citep{janzing2020feature}",1.0,1,Is the xAI able to explain the model correctly despite a non-uniform statistically-dependent distribution of the data?,"check if the explanation change when the distribution change. check if non-uniform,  statistically dependent distributions affect the explanation",demonstrate the effect of data distribution / causal inference,-,non-uniform /statistically dependent,-,1650.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,0.0
,x0_plus_x1_distrib_non_uniform_stat_indep,stability,"To assess the impact of changing the inputs of f... This way, we are able to talk about a hypothetical scenario where the inputs are changed compared
to the true features. \citep{janzing2020feature}",1.0,1,Is the xAI able to explain the model correctly despite a non-uniform distribution of the data?,check if the explanation change when the distribution change. check if non-uniform distributions affect the explanationnon,demonstrate the effect of data distribution / causal inference,-,non-uniform /statistically independent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,0.0
,x0_plus_x1_distrib_uniform_stat_dep,stability,"To assess the impact of changing the inputs of f... This way, we are able to talk about a hypothetical scenario where the inputs are changed compared
to the true features. \citep{janzing2020feature}",1.0,1,Is the xAI able to explain the model correctly despite a statistically-dependent distribution of the data?,check if the explanation change when the distribution change. check if statistically dependent distributions affect the explanationnon,demonstrate the effect of data distribution / causal inference. The example was given in both https://arxiv.org/pdf/1905.03151.pdf and \cite{janzing2020feature},-,uniform /statistically dependent,-,10000.0,XGBRegressor,janzing2020feature,rewritten,https://github.com/Karim-53/Compare-xAI/tree/main/tests/x0_plus_x1.py,"{0,1}",,,0.0
,mnist,stress,The test is adapted from \citep{covert2020understanding}. It is a stress test because of the high number of input features,1.0,1,Is the xAI able to detect all dummy (constant and useless) pixels?,simply train and explain a MLP,We agree that important pixels are in the center of the image,-,MNIST,https://www.openml.org/d/554,70000.0,MLP,covert2020understanding,https://github.com/iancovert/sage-experiments/blob/main/experiments/univariate_predictors.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,image,ratio of constant pixels detected non dummy divided by the number of constant pixels,,1.0
,fooling_perturbation_alg,fragility,"fragility includes all adversarial attacks, see \citep{ghorbani2019interpretation}",1.0,1,Is the xAI affected by an adversarial attack against perturbation-based algorithms?,explain the following  function: if the input is from the dataset then the output is from a biased model. if not then the output is from a fair model.,"exploit a vulnerability in model-agnostic xai that use feature perturbation: adversarial attack to lower the feature importance of a specific feature ### Setup: Let's begin by examining the COMPAS data set. This data set consists of defendent information from Broward Couty, Florida. Let's suppose that some adversary wants to _mask_ baised or racist behavior on this data set.",-,https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv  https://www.kaggle.com/datasets/danofer/compass,https://github.com/propublica/compas-analysis/blob/master/compas-scores-two-years.csv  https://www.kaggle.com/datasets/danofer/compass,,function,lakkaraju2020fool,https://github.com/dylan-slack/Fooling-LIME-SHAP/blob/master/COMPAS_Example.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,categorical/scalars,Race should be the most important feature is the f importance of Race as the correct value or is it affected by the adversarial attack. How to: corrupt out of distribution predictions => works only on model agnostic xai.  Adversarial black box provided: (if perturbed return prediction from fair model    else return the prediction from the real biased model)   https://github.com/SinaMohseni/Awesome-XAI-Evaluation/blob/master/tables/main-table.csv,,1.0
,counterexample_dummy_axiom,simplicity,assigning an importance of zero to dummy feature reflect the model behavior (Fidelity) but also helps the data scientist to quickly understand the model (Simplicity),1.0,1,Is the xAI able to detect unused input features?,train a model with one feature b. test if feature a has an importance different from 0,counter example used to verify that SHAP CES do not satisfy the dummy axiom. BSHAP succeed in this test.,-,synthetic,-,20000.0,function,sundararajan2020many,,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,continuous,,,
,a_and_b_or_c,fidelity,"Same reason as cough_and_fever_10_90, A's effect on the output is higher than b or c",1.0,1,Can the xAI detect that input feature 'A' is more important than 'B' or 'C'?,model learn the following equation : A and (B or C). Explanation should prove that A is more important,"This is a baseline test that the xAI should succeed in all cases. Model: A and (B or C). Goal: make sure that A is more important than B, C. Noise: even if the model output is not == 1. still we expect the xai to give a correct answer => no noise. Score function: if A is the most important feature → return 1.		If A is the 2nd most important feature → return 0.5     i.e. 1- (1 / nb of feature more important than A).  If A is the last one: return 0 (completely wrong)",-,synthetic,-,20000.0,XGBRegressor,-,-,https://github.com/Karim-53/Compare-xAI/tree/main/tests/a_and_b_or_c.py,binary,,-,0.0
,stress_nb_features,stress,high number of tokens,1.0,1,is the xAI able to explain NLP tasks despite a high number of feature values (here 1555 tokens)?,train an NLP model. specific token get a high influence on the output as proven in cited papers.,,-,imdb,http://www.aclweb.org/anthology/P11-1015,,LogisticRegression,lundberg2017unified,https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.ipynb,https://github.com/Karim-53/Compare-xAI/tree/main/tests/,tokens,"1 if the chosen token is in the top 4.
0 if it is in the last 25% least important feature",,
