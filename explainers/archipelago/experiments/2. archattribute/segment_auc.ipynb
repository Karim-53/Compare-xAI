{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from experiment_utils import *\n",
    "\n",
    "sys.path.append(\"../../src\")\n",
    "from explainer import Explainer\n",
    "from application_utils.image_utils import *\n",
    "from application_utils.utils_torch import ModelWrapperTorch\n",
    "\n",
    "sys.path.append(\"../../baselines/integrated_gradients\")\n",
    "import ig, ig_utils\n",
    "\n",
    "sys.path.append(\"../../baselines/shapley_interaction_index\")\n",
    "from si_explainer import SiExplainer\n",
    "\n",
    "sys.path.append(\"../../baselines/shapley_taylor_interaction_index\")\n",
    "from sti_explainer import StiExplainer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"archattribute\"] # for analysis code to run smoothly, use one method per experiment run\n",
    "save_path = \"analysis/results/segment_auc_archattribute.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True).to(device).eval();\n",
    "model_wrapper = ModelWrapperTorch(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.17s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/meladyfs/newyork/datasets/mscoco'\n",
    "data_type = \"val2017\"\n",
    "coco_to_i1k_path = \"processed_data/image_data/coco_to_i1k_map.pickle\"\n",
    "annFile='{}/annotations/instances_{}.json'.format(data_dir, data_type)\n",
    "coco=COCO(annFile)\n",
    "i1k_idx_to_cat, valid_cat_ids, cat_map = prep_imagenet_coco_conversion(coco, data_dir=data_dir, data_type=data_type, coco_to_i1k_path=coco_to_i1k_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archattribute(model, image_tensor, mask_tensor, model_target_idx, device):\n",
    "    predictions_island = model(image_tensor*mask_tensor.to(device))\n",
    "    predictions_baseline = model(torch.zeros_like(mask_tensor).to(device))\n",
    "    predictions = (predictions_island - predictions_baseline)\n",
    "    att_score = predictions.data.cpu().numpy()[0][model_target_idx]\n",
    "    return att_score\n",
    "\n",
    "def integrated_gradients(model, image_tensor, model_target_idx, device):\n",
    "    ig_score = ig.integrated_gradients(image_tensor.squeeze().cpu().numpy(), model, model_target_idx, ig_utils.get_gradients, None, device, steps=50)\n",
    "    return ig_score\n",
    "\n",
    "def shapley_interaction_index(image, baseline, segments, S, model_target_idx, seed =None, num_T=20):\n",
    "    xf = ImageXformer(image, baseline, segments)\n",
    "    e = SiExplainer(model_wrapper, data_xformer=xf, output_indices=model_target_idx, batch_size=20, seed=seed)\n",
    "    att = e.attribution(S, num_T)\n",
    "    return att\n",
    "\n",
    "def shapley_taylor_interaction_index(image, baseline, segments, S, model_target_idx, max_order=2, num_orderings=20, seed=None):\n",
    "\n",
    "    def subset_before(S, ordering, ordering_dict):\n",
    "        end_idx = min(ordering_dict[s] for s in S)\n",
    "        return ordering[:end_idx]\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    xf = ImageXformer(image, baseline, segments)\n",
    "    e = StiExplainer(model_wrapper, data_xformer=xf, output_indices=model_target_idx, batch_size=20)\n",
    "\n",
    "    num_feats = len(np.unique(segments))\n",
    "    att = 0\n",
    "    for ordering in range(num_orderings):\n",
    "        ordering = np.random.permutation(list(range(num_feats)))\n",
    "        ordering_dict = {ordering[i]: i for i in range(len(ordering))}\n",
    "    \n",
    "        if len(S) == max_order:\n",
    "            T = subset_before(S, ordering, ordering_dict)\n",
    "            att_inst = e.attribution(S, T)\n",
    "        else:\n",
    "            att_inst = e.attribution(S, [])\n",
    "            \n",
    "        att += att_inst\n",
    "                \n",
    "    return att/num_orderings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as handle:\n",
    "        results = pickle.load(handle)\n",
    "else:\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plots = False\n",
    "max_imgs_per_category = 500\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "seenImgs = set()\n",
    "img_count = 0\n",
    "\n",
    "for cat_id in tqdm(valid_cat_ids):\n",
    "    # get image ids corresponding to a category\n",
    "    imgIds = coco.getImgIds(catIds=[cat_id] );\n",
    "\n",
    "    for i, imgId in enumerate(imgIds):\n",
    "        \n",
    "        if imgId in seenImgs:\n",
    "            continue\n",
    "        seenImgs.add(imgId)\n",
    "        \n",
    "        # load the image metadata\n",
    "        img = coco.loadImgs(imgId)[0]\n",
    "        # load the annotation ids for this image\n",
    "        annIds = coco.getAnnIds(imgIds=img['id'], catIds=valid_cat_ids, iscrowd=None)\n",
    "\n",
    "        # if this image and all of its annotations have already been examined for each method, skip\n",
    "        if imgId in results and all(len(results[imgId][\"est\"][m]) == len(annIds) for m in methods):\n",
    "            continue\n",
    "            \n",
    "        # load the actual image\n",
    "        I = io.imread('%s/images/%s/%s'%(data_dir,data_type,img['file_name']))\n",
    "\n",
    "        # if grey, convert to RGB\n",
    "        if len(I.shape) == 2:\n",
    "            I = gray2rgb(I)\n",
    "            \n",
    "        if show_plots:\n",
    "            plt.imshow(I); plt.axis('off')\n",
    "\n",
    "        image = Image.fromarray(I)\n",
    "        image, image_tensor = transform_img(I, preprocess)\n",
    "        top_model_class_idxs = model(image_tensor.to(device)).data.cpu().numpy()[0].argsort()[::-1]\n",
    "\n",
    "        # select the top predicted class that intersects with coco classes\n",
    "        for i in top_model_class_idxs:\n",
    "            if i in i1k_idx_to_cat:\n",
    "                model_target_idx = i\n",
    "                break\n",
    "\n",
    "        # use superpixel segmenting with SI or STI\n",
    "        if any( m in {\"si\", \"sti\"} for m in methods):\n",
    "            segments = quickshift(image, kernel_size=3, max_dist=300, ratio=0.2)\n",
    "\n",
    "        # actually load the annotations\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        for method in methods:\n",
    "                \n",
    "            results[imgId] = {\"ref\": [], \"est\": {}}\n",
    "            results[imgId][\"est\"][method] = []\n",
    "\n",
    "            if method == \"integrated_gradients\":\n",
    "                ig_score =  integrated_gradients(model, image_tensor, model_target_idx, device)\n",
    "\n",
    "            for ann in (anns):\n",
    "\n",
    "                assert(ann[\"category_id\"] in valid_cat_ids)\n",
    "                \n",
    "                # get the mask for this annotation\n",
    "                mask = coco.annToMask(ann)\n",
    "\n",
    "                # transform (resize) the mask through torch, but maintain a mask with black background (dont normalize)\n",
    "                mask_resize = np.tile(np.expand_dims(mask, 2), 3).astype(np.uint8)\n",
    "                mask_orig, mask_tensor = transform_img(mask_resize, preprocess_mask)\n",
    "                \n",
    "                # there must be something to show through the mask after resizing \n",
    "                if math.isnan(mask_tensor.sum().item()): \n",
    "                    continue\n",
    "                    \n",
    "                # process how the mask corresponds to superpixel segments\n",
    "                if method in {\"si\", \"sti\"}:\n",
    "                    inter = match_segments_and_mask(segments, mask_orig)\n",
    "                    # require that with SI and STI, only two segments are selected for pairwise interaction attribution \n",
    "                    if len(inter) != 2:\n",
    "                        continue\n",
    "\n",
    "                if show_plots:\n",
    "                    plt.figure(figsize = (6,6))\n",
    "                    plt.axis('off')\n",
    "                    plt.imshow((image*mask_orig)/2+0.5)\n",
    "                    plt.show()\n",
    "\n",
    "                # apply the different methods for the model_target_idx, the \"top\" prediction on the original image\n",
    "                if method == \"archattribute\":\n",
    "                    att_score = archattribute(model, image_tensor, mask_tensor, model_target_idx, device)\n",
    "                elif method == \"integrated_gradients\":\n",
    "                    att_score = (mask_tensor.cpu().numpy()*ig_score).sum()\n",
    "                elif method == \"si\":\n",
    "                    att_score = shapley_interaction_index(image, np.zeros_like(image), segments, inter, model_target_idx, seed=img_count)\n",
    "                elif method == \"sti\":\n",
    "                    att_score = shapley_taylor_interaction_index(image, np.zeros_like(image), segments, inter, model_target_idx, seed=img_count)\n",
    "                    \n",
    "\n",
    "                results[imgId][\"est\"][method].append(att_score)\n",
    "\n",
    "                # if the annotation does not belong to the category of model_target_idx, ground truth is 0, else it is 1\n",
    "                if cat_map[ann[\"category_id\"]] not in i1k_idx_to_cat[model_target_idx]: # \"in\" handles the vase case, which maps to two coco categories\n",
    "                    results[imgId][\"ref\"].append(0)\n",
    "                else:\n",
    "                    results[imgId][\"ref\"].append(1)\n",
    "            \n",
    "        # save attribution results and corresponding ground truth\n",
    "        if img_count % 1 == 0:\n",
    "            with open(save_path, 'wb') as handle:\n",
    "                pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        img_count += 1\n",
    "            \n",
    "t1 = time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
