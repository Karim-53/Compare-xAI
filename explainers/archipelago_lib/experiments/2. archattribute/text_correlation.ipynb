{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "# archattribute\n",
    "sys.path.append(\"../../src\")\n",
    "from explainer import Archipelago\n",
    "from application_utils.text_utils import *\n",
    "from application_utils.text_utils_torch import BertWrapperTorch\n",
    "\n",
    "# difference\n",
    "sys.path.append(\"../../baselines/difference\")\n",
    "from diff_explainer import DiffExplainer\n",
    "\n",
    "# integrated gradients and integrated hessians\n",
    "# used the IG implemented in the IH package\n",
    "sys.path.append(\"../../baselines/integrated_hessians\")\n",
    "from path_explain import utils\n",
    "from embedding_explainer_bert import EmbeddingExplainerTF\n",
    "from application_utils.text_utils_tf import BertWrapperIH\n",
    "\n",
    "# scd-soc\n",
    "sys.path.append(\"../../baselines/scd_soc\")\n",
    "sys.path.append(\"../../baselines/scd_soc/hiexpl\")\n",
    "import helper\n",
    "\n",
    "sys.path.append(\"../../baselines/shapley_interaction_index\")\n",
    "from si_explainer import SiExplainer\n",
    "\n",
    "sys.path.append(\"../../baselines/shapley_taylor_interaction_index\")\n",
    "from sti_explainer import StiExplainer\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'analysis/results/phrase_corr_archattribute.pickle'\n",
    "# save_path = 'analysis/results/word_corr_archattribute.pickle'\n",
    "\n",
    "methods = [\"archattribute\"] # for analysis code to run smoothly, use one method per experiment run\n",
    "\n",
    "sti_max_order = 2\n",
    "ig_baseline_token = \"[PAD]\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'sst-2'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_path = \"../../downloads/pretrained_bert\"\n",
    "\n",
    "if any(m in {\"archattribute\", \"difference\", \"si\", \"sti\"} for m in methods):\n",
    "    import torch\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch_model = helper.get_bert(model_path, device)\n",
    "    model_wrapper = BertWrapperTorch(torch_model, device)\n",
    "    class_idx = 1\n",
    "\n",
    "if any(m in {\"integrated_gradients\", \"integrated_hessians\"} for m in methods):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_path, from_pt=True)\n",
    "    model_wrapper_ih = BertWrapperIH(model)\n",
    "    class_idx = 1\n",
    "if any(m in {\"soc\", \"scd\"} for m in methods):\n",
    "    import torch\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch_model = helper.get_bert(model_path, device)\n",
    "    lm_path = \"../../downloads/pretrained_hiexpl_lm/best_snapshot_devloss_11.708949835404105_iter_2000_model.pt\"\n",
    "    lm_model = helper.get_lm_model(lm_path, device.index)\n",
    "\n",
    "    if \"soc\" in methods:\n",
    "        soc_algo = helper.get_hiexpl(\"soc\", torch_model, \"\", tokenizer, device, sample_num=20, lm_model=lm_model)\n",
    "    if \"scd\" in methods:\n",
    "        scd_algo = helper.get_hiexpl(\"scd\", torch_model, \"\", tokenizer, device, sample_num=20, lm_model=lm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Phase Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = 'processed_data/text_data/subtree_allphrase_nosentencelabel.pickle'\n",
    "# gt_file = 'processed_data/text_data/subtree_single_token.pickle'\n",
    "\n",
    "with open(gt_file, 'rb') as handle:\n",
    "    phrase_gt_splits = pickle.load(handle)\n",
    "\n",
    "phrase_gt = phrase_gt_splits[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Attributions for Each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spans(spans):\n",
    "    spans_input = []\n",
    "    for span in spans:\n",
    "        spans_input.append(tuple(np.arange(span[0], span[1]+1) + 1))\n",
    "    return spans_input\n",
    "\n",
    "def inv_span(span_output):\n",
    "    span_indices = np.array(span_output) - 1\n",
    "    span =  (span_indices[0], span_indices[-1])\n",
    "    return span\n",
    "\n",
    "def get_index_map(spans):\n",
    "    # only tested for contiguous pairs, which is sufficient for this eval\n",
    "    index_map = {}\n",
    "    accounted = set()\n",
    "    break_cont=False\n",
    "    for i in range(len(spans)):\n",
    "        for j in range(i+1, len(spans)):\n",
    "\n",
    "            if spans[i] in accounted or spans[j] in accounted: continue\n",
    "            intersect = list(set(spans[i]) & set(spans[j]))\n",
    "            if intersect:\n",
    "                assert(len(intersect) == 1)\n",
    "                if intersect[0] not in index_map:\n",
    "                    index_map[intersect[0]] = [] \n",
    "                index_map[intersect[0]].extend([spans[i][0],spans[j][1]])\n",
    "                accounted.add(spans[i])\n",
    "                accounted.add(spans[j])\n",
    "\n",
    "    for span in spans:\n",
    "        if span not in accounted:\n",
    "            index_map[span[0]] = [span[1]]\n",
    "    return index_map\n",
    "\n",
    "\n",
    "def archattribute(sentence, spans):\n",
    "    baseline_token = \"_\"\n",
    "    text_ids, baseline_ids = get_input_baseline_ids(sentence, baseline_token, tokenizer)\n",
    "    xf = TextXformer(text_ids, baseline_ids) \n",
    "    apgo = Archipelago(model_wrapper, data_xformer=xf, output_indices=class_idx, batch_size=20)\n",
    "    spans_input = transform_spans(spans)\n",
    "    arch_atts = apgo.archattribute( spans_input )      \n",
    "    return arch_atts\n",
    "\n",
    "def difference(sentence, spans):\n",
    "    baseline_token = \"_\"\n",
    "    text_ids, baseline_ids = get_input_baseline_ids(sentence, baseline_token, tokenizer)\n",
    "    xf = TextXformer(text_ids, baseline_ids) \n",
    "    d = DiffExplainer(model_wrapper, data_xformer=xf, output_indices=class_idx, batch_size=20)\n",
    "    spans_input = transform_spans(spans)\n",
    "    diff_atts = d.difference_attribution( spans_input )         \n",
    "    return diff_atts\n",
    "\n",
    "def integrated_gradients(sentence, spans, baseline_token):\n",
    "    # use the default baseline_token from the official IG repo\n",
    "    # there are minor differences with the \"_\" token we use\n",
    "    batch_predictions, orig_token_list, batch_embedding, baseline_embedding, attention_mask = model_wrapper_ih.get_predictions_extra([sentence], tokenizer, baseline_token=baseline_token)\n",
    "    explainer = EmbeddingExplainerTF(model_wrapper_ih.prediction_model)\n",
    "    ig_atts = explainer.attributions(inputs=batch_embedding,\n",
    "                                          baseline=baseline_embedding,\n",
    "                                          batch_size=20,\n",
    "                                          num_samples=50,\n",
    "                                          use_expectation=False,\n",
    "                                          output_indices=1,\n",
    "                                          verbose=False,\n",
    "                                          attention_mask=attention_mask)\n",
    "    ig_atts = ig_atts.flatten()[1:-1]\n",
    "    ig_span_atts = {}\n",
    "    for span in spans:\n",
    "        ig_span_atts[span] = np.sum(ig_atts[np.arange(span[0], span[1]+1) ])\n",
    "    return ig_span_atts\n",
    "\n",
    "def integrated_hessians(sentence, spans):\n",
    "    # use the default baseline_token from the official IH repo\n",
    "    # there are minor differences with the \"_\" token we use\n",
    "    batch_predictions, orig_token_list, batch_embedding, baseline_embedding, attention_mask = model_wrapper_ih.get_predictions_extra([sentence], tokenizer)\n",
    "    explainer = EmbeddingExplainerTF(model_wrapper_ih.prediction_model)\n",
    "    index_map = get_index_map(spans)\n",
    "        \n",
    "    ih_atts = {}\n",
    "    for i in index_map:\n",
    "        ih_atts_slice = explainer.interactions(inputs=batch_embedding,\n",
    "                                              baseline=baseline_embedding,\n",
    "                                              batch_size=20,\n",
    "    #                                          num_samples=2,\n",
    "                                              num_samples=50,\n",
    "                                              use_expectation=False,\n",
    "                                              output_indices=1,\n",
    "                                              verbose=False,\n",
    "                                              attention_mask=attention_mask,\n",
    "                                           interaction_index = i+1\n",
    "                                        )\n",
    "        for j in index_map[i]:\n",
    "            ih_span = tuple(sorted((i,j)))\n",
    "            ih_atts[ih_span] = ih_atts_slice[0,j+1]\n",
    "    return ih_atts\n",
    "\n",
    "def scd_soc(sentence, spans, algo):\n",
    "    contribs, tokens = helper.explain_sentence(sentence, algo, tokenizer, spans = spans)\n",
    "    contrib_dict = {}\n",
    "    for sp in contribs:\n",
    "        contrib_dict[tuple(np.array(sp)-1)] = contribs[sp]\n",
    "    return contrib_dict\n",
    "\n",
    "def shapley_interaction_index(sentence, spans, seed =None,  num_T=20):\n",
    "    baseline_token = \"_\"\n",
    "    text_ids, baseline_ids = get_input_baseline_ids(sentence, baseline_token, tokenizer)\n",
    "    xf = TextXformer(text_ids, baseline_ids) \n",
    "    e = SiExplainer(model_wrapper, data_xformer=xf, output_indices=class_idx, batch_size=20, seed=seed)\n",
    "    atts = {}\n",
    "    for span in spans:\n",
    "        S = list(range(span[0]+1, span[1]+2))\n",
    "        att = e.attribution(S, num_T)\n",
    "        atts[span] = att\n",
    "    return atts\n",
    "\n",
    "def shapley_taylor_interaction_index(sentence, spans, max_order=2, seed=None, num_orderings=20):\n",
    "    \n",
    "    def subset_before(S, ordering, ordering_dict):\n",
    "        end_idx = min(ordering_dict[s] for s in S)\n",
    "        return ordering[:end_idx]\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    baseline_token = \"_\"\n",
    "    text_ids, baseline_ids = get_input_baseline_ids(sentence, baseline_token, tokenizer)\n",
    "\n",
    "\n",
    "    xf = TextXformer(text_ids, baseline_ids) \n",
    "    e = StiExplainer(model_wrapper, data_xformer=xf, output_indices=class_idx, batch_size=20)\n",
    "    \n",
    "    atts = {}\n",
    "    for i in range(num_orderings):\n",
    "        ordering = np.random.permutation(list(range(len(text_ids))))\n",
    "        ordering_dict = {ordering[i]: i for i in range(len(ordering))}\n",
    "\n",
    "        for span in spans:\n",
    "            S = list(range(span[0]+1, span[1]+2))\n",
    "\n",
    "            if len(S) == max_order:\n",
    "                T = subset_before(S, ordering, ordering_dict)\n",
    "                att = e.attribution(S, T)\n",
    "            else:\n",
    "                att = e.attribution(S, [])\n",
    "\n",
    "            if span not in atts:\n",
    "                atts[span] = 0\n",
    "            atts[span] += att\n",
    "            \n",
    "    for span in atts:\n",
    "        atts[span] = atts[span] / num_orderings\n",
    "    return atts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(save_path):\n",
    "    with open(save_path, 'rb') as handle:\n",
    "        p_dict = pickle.load(handle)\n",
    "    ref_sanity = p_dict[\"ref\"]\n",
    "    ref = {}\n",
    "    est_methods = p_dict[\"est\"]\n",
    "else:\n",
    "    ref = {}\n",
    "    est_methods = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2210/2210 [06:05<00:00,  6.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for k in methods:\n",
    "    if k not in est_methods:\n",
    "        est_methods[k] = {}\n",
    "\n",
    "for s_idx, phrase_dict in enumerate(tqdm(phrase_gt)):\n",
    "    \n",
    "    sentence = phrase_dict[\"sentence\"]\n",
    "    tokens = phrase_dict[\"tokens\"]\n",
    "    subtrees = phrase_dict[\"subtrees\"]\n",
    "    att_len = len(tokens)\n",
    "    \n",
    "    span_to_label = {}\n",
    "    for subtree in subtrees:        \n",
    "        span_to_label[subtree[\"span\"]] = subtree[\"label\"]\n",
    "        \n",
    "    spans = list(span_to_label.keys())\n",
    "    \n",
    "    if s_idx not in ref:\n",
    "        ref[s_idx] = [span_to_label[sp] for sp in spans]  \n",
    "\n",
    "    if all((s_idx in est_methods[m]) for m in methods):\n",
    "        print(\"skip\", s_idx)\n",
    "        continue\n",
    "        \n",
    "    results = {}\n",
    "    for method in methods:\n",
    "        if method == \"archattribute\":\n",
    "            prop_atts = archattribute(sentence, spans)\n",
    "            results[method] = {inv_span(s): prop_atts[s] for s in prop_atts}\n",
    "\n",
    "        elif method == \"difference\":\n",
    "            diff_atts = difference(sentence, spans)\n",
    "            results[method] = {inv_span(s): diff_atts[s] for s in diff_atts}\n",
    "\n",
    "        elif method == \"integrated_gradients\":\n",
    "            results[method] = integrated_gradients(sentence, spans, ig_baseline_token)\n",
    "\n",
    "        elif method == \"integrated_hessians\":\n",
    "            results[method] = integrated_hessians(sentence, spans)\n",
    "\n",
    "        elif method == \"soc\":\n",
    "            soc_atts = scd_soc(sentence, spans, soc_algo)\n",
    "            results[method] = soc_atts\n",
    "\n",
    "        elif method == \"scd\":\n",
    "            scd_atts = scd_soc(sentence, spans, scd_algo)\n",
    "            results[method] = scd_atts\n",
    "\n",
    "        elif method == \"si\":\n",
    "            si_atts = shapley_interaction_index(sentence, spans, seed = s_idx)\n",
    "            results[method] = si_atts\n",
    "\n",
    "        elif method == \"sti\":\n",
    "            sti_atts = shapley_taylor_interaction_index(sentence, spans, max_order = sti_max_order, seed = s_idx)\n",
    "            results[method] = sti_atts\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    for k in results:\n",
    "        est_vec = []\n",
    "        for span in spans:\n",
    "            est_vec.append(results[k][span])\n",
    "\n",
    "        est_methods[k][s_idx] = est_vec\n",
    "                \n",
    "    if (s_idx+1) % 3 == 0:      \n",
    "        with open(save_path, 'wb') as handle:\n",
    "            pickle.dump({\"est\": est_methods, \"ref\": ref}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(save_path, 'wb') as handle:\n",
    "    pickle.dump({\"est\": est_methods, \"ref\": ref}, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
