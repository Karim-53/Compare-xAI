import sys
import traceback

import numpy as np
import shap as shap_lib

from explainers.explainer_superclass import Explainer, UnsupportedModelException
from src.utils import get_importance


class Shap(Explainer, name='shap'):  # todo finish implement this
    expected_values = None
    attribution = None
    importance = None

    description = """shap is unifying all other methods (Deeplift, limeshapley regression…)"""

    def __init__(self, f, X, **kwargs):
        self.f = f
        self.X = X
        self.explainer = shap_lib.Explainer(self.f, self.X)

    def explain(self, x, **kwargs):
        shap_values = self.explainer(x)
        self.expected_values, shap_values = shap_values.base_values, shap_values.values
        return shap_values


class KernelShap(Explainer,    name = 'kernel_shap'):

    supported_models = ('model_agnostic',)
    output_attribution = True
    output_importance = True  # inferred

    def __init__(self, trained_model, X, predict_func, **kwargs):
        super().__init__()
        self.trained_model = trained_model
        self.predict_func = predict_func
        self.reference_dataset = np.array(X, dtype='float64')
        print(self.reference_dataset.dtype)
        # todo find a way to restrict reference dataset size
        # print(type(self.reference_dataset))
        # print(self.reference_dataset.shape)
        self.predict_func(self.reference_dataset)  # just a test
        # todo improve this
        #  WARNING  Using 4629 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.
        try:
            self.explainer = shap_lib.KernelExplainer(self.predict_func, self.reference_dataset, **kwargs)
        except Exception as e:
            exc_info = sys.exc_info()
            traceback.print_exception(*exc_info)
            print(e)

    def explain(self, dataset_to_explain, **kwargs):
        self.interaction = None
        self.expected_values = self.explainer.expected_value
        shap_values = self.explainer.shap_values(np.array(dataset_to_explain))
        self.attribution = np.array(shap_values)
        self.importance = get_importance(self.attribution)


class TreeShap(Explainer, name = 'tree_shap'):
    library_version = shap_lib.__version__  # todo [after acceptance] record library name and version automatically

    supported_models = ('tree_based',)
    output_attribution = True
    output_importance = True  # inferred
    description = """ <<In an email exchange, Scott Lundberg clarified that the implicit assumption is that the
features are distributed according to ”the distribution generated by the tree”.>> See https://arxiv.org/pdf/1908.08474.pdf  I don t know if this still holds or was fixed. """

    def __init__(self, trained_model, df_reference, predict_func, **kwargs):
        super().__init__()
        self.trained_model = trained_model
        self.f = predict_func
        if self.f is None:
            print('why there is no predict_func ?')
            self.f = trained_model.predict
        self.df_reference = df_reference
        try:
            self.explainer = shap_lib.TreeExplainer(self.trained_model, self.df_reference, **kwargs)
        except Exception as e:
            if 'Model type not yet supported by TreeExplainer' in str(e):
                raise UnsupportedModelException(str(e))
            else:
                raise

    def explain(self, dataset_to_explain, **kwargs):
        shap_values = self.explainer(dataset_to_explain,
                                     check_additivity=False)  # todo after acceptance reort all problems to shap due to additivity
        self.expected_values = shap_values.base_values
        self.attribution = shap_values.values
        self.importance = get_importance(self.attribution)


class Permutation(Explainer, name = 'permutation'):
    library_version = shap_lib.__version__  # todo [after acceptance] record library name and version automatically

    def __init__(self, predict_func, X_reference, **kwargs):
        super().__init__()

        # use an independent masker
        masker = shap_lib.maskers.Independent(X_reference)
        print('masker', masker)

        # build the explainers
        self.explainer = shap_lib.explainers.Permutation(predict_func, masker)
        print('self.explainer', self.explainer)
        # try:
        #     self.explainer = shap_lib.TreeExplainer(self.trained_model, self.df_reference, **kwargs)
        # except Exception as e:
        #     if 'Model type not yet supported by TreeExplainer' in str(e):
        #         raise UnsupportedModelException(str(e))
        #     else:
        #         raise

    def explain(self, dataset_to_explain, **kwargs):
        attribution: shap_lib._explanation.Explanation = self.explainer(dataset_to_explain)
        # self.expected_values = shap_values.base_values
        self.attribution = attribution.values
        self.importance = get_importance(self.attribution)


class Partition(Explainer):
    def __init__(self, predict_func, X_reference, **kwargs):
        masker = shap_lib.maskers.Partition(X_reference)
        self.explainer = shap_lib.explainers.Partition(predict_func, masker)

    def explain(self, dataset_to_explain, **kwargs):
        attribution: shap_lib._explanation.Explanation = self.explainer(dataset_to_explain)
        self.attribution = attribution.values
        self.importance = get_importance(self.attribution)


class TreeShapApproximation(Explainer):
    def __init__(self, trained_model, X_reference, **kwargs):
        masker = shap_lib.maskers.Independent(X_reference)
        try:
            self.explainer = shap_lib.explainers.Tree(trained_model, masker, approximate=True)
        except Exception as e:
            if 'Model type not yet supported by TreeExplainer' in str(e):
                raise UnsupportedModelException(str(e))
            else:
                raise

    def explain(self, dataset_to_explain, **kwargs):
        attribution: shap_lib._explanation.Explanation = self.explainer(dataset_to_explain)
        self.attribution = attribution.values
        self.importance = get_importance(self.attribution)


class ExactShapleyValues(Explainer):
    def __init__(self, predict_func, X_reference, **kwargs):
        masker = shap_lib.maskers.Independent(X_reference)
        self.explainer = shap_lib.explainers.Exact(predict_func, masker)

    def explain(self, dataset_to_explain, **kwargs):
        attribution: shap_lib._explanation.Explanation = self.explainer(dataset_to_explain)
        self.attribution = attribution.values
        self.importance = get_importance(self.attribution)


class PermutationPartition(Explainer):
    def __init__(self, predict_func, X_reference, **kwargs):
        masker = shap_lib.maskers.Partition(X_reference)
        self.explainer = shap_lib.explainers.Permutation(predict_func, masker)

    def explain(self, dataset_to_explain, **kwargs):
        attribution: shap_lib._explanation.Explanation = self.explainer(dataset_to_explain)
        self.attribution = attribution.values
        self.importance = get_importance(self.attribution)


if __name__ == '__main__':
    print(shap_lib.__version__)

    import numpy as np
    from sklearn.model_selection import train_test_split
    import xgboost
    # import shap
    # import shap.benchmark
    #
    # build the model
    model = xgboost.XGBRegressor(n_estimators=1000, subsample=0.3)
    X, y = shap_lib.datasets.boston()
    X = X.values
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=False)

    # define the benchmark evaluation sample set
    X_eval = X_test[:]
    y_eval = y_test[:]

    explainer = Permutation(model.predict, X_train)
    print(explainer)
    #
    # # use an independent masker
    # masker = shap.maskers.Independent(X_train)
    # pmasker = shap.maskers.Partition(X_train)
    #
    # # build the explainers
    # explainers = [
    #     ("Permutation", shap.explainers.Permutation(model.predict, masker)),
    #     ("Permutation part.", shap.explainers.Permutation(model.predict, pmasker)),
    #     ("Partition", shap.explainers.Partition(model.predict, pmasker)),
    #     ("Tree", shap.explainers.Tree(model, masker)),
    #     ("Tree approx.", shap.explainers.Tree(model, masker, approximate=True)),
    #     ("Exact", shap.explainers.Exact(model.predict, masker)),
    #     ("Random", shap.explainers.other.Random(model.predict, masker))
    # ]
    #
    # # # dry run to get all the code warmed up for valid runtime measurements
    # for name, exp in explainers:
    #     out = exp(X_eval[:5])
    explainer.explain(X_eval[:5])
    print('out', explainer.attribution)
    # # explain with all the explainers
    # attributions = [(name, exp(X_eval)) for name, exp in explainers]
    # # Partition explainer: 128it [00:15,  3.01it/s]
    # # Exact explainer: 128it [00:39,  2.40it/s]
    # # Run the benchmarks
    # results = {}
    #
    # smasker = shap.benchmark.ExplanationError(
    #     masker, model.predict, X_eval
    # )
    # results["explanation error"] = [smasker(v, name=n) for n,v in attributions]
    #
    # ct = shap.benchmark.ComputeTime()
    # results["compute time"] = [ct(v, name=n) for n,v in attributions]
    #
    # for mask_type, ordering in [("keep", "positive"), ("remove", "positive"), ("keep", "negative"), ("remove", "negative")]:
    #     smasker = shap.benchmark.SequentialMasker(
    #         mask_type, ordering, masker, model.predict, X_eval
    #     )
    #     results[mask_type + " " + ordering] = [smasker(v, name=n) for n,v in attributions]
    #
    # cmasker = shap.maskers.Composite(masker, shap.maskers.Fixed())
    # for mask_type, ordering in [("keep", "absolute"), ("remove", "absolute")]:
    #     smasker = shap.benchmark.SequentialMasker(
    #         mask_type, ordering, cmasker, lambda X, y: (y - model.predict(X))**2, X_eval, y_eval
    #     )
    #     results[mask_type + " " + ordering] = [smasker(v, name=n) for n,v in attributions]
